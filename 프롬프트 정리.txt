genetic tree를 구현해줘. 아래의 명세를 모두 반영해야 한다.
- 해당 GA tree의 목적은 자동 트레이딩을 위한 코드이다.
- tree의 분기는 무조건 비교 구문으로 구성된다.
-- tree의 분기는 무조건 2개의 형식으로 구분된다.
--- 하나의 노드의 자식 노드는 여러개가 될 수 있다. (제한 없음)
--- feature >< number or feature1 >< feature2
--- 형식의 feature는 미리 지정해준다.
--- ex)   feature_num     = ['RSI', 'ATR', 'WR'] 
  feature_pair    = ['SMA', 'EMA', 'BB_upper','BB_lower']
--- 즉, 숫자와 비교하는 feature와 feature끼리 비교하는 feature들의 그룹을 미리 지정해줄 예정이다.
--- feature_num의 경우, 해당 데이터의 최대 및 최소 값도 같이 주어질 예정이다.
--- GA의 변이는 tree의 분기를 늘리거나 줄이거나 해당 분기를 위의 조건에 맞는 한에서 변경한다.
-- 트리의 root 바로 아래 노드는 3개의 노드가 존재하며, 각각 현재 포지션이 ['LONG', 'HOLD', 'SHORT']인지 구분한다.
--- crossover는 기존 트리에서 일정 부분의 branch 전체를 선택해서 crossover를 진행한다.
--- crossover되는 branch에 하위 branch가 존재한다면, 통째로 crossover가 된다.
--- 자식 노드가 여러개일 경우에는, 하나의 자식 노드만 충족해도 해당 자식 노드들의 모든 경우를 탐색한다.
---- 예를 들어 자식 노드 1, 2, 3이 있을 경우 1과 3의 경우가 충족되었다면, 1과 3의 tree로 진행한다.
--- 진행한 트리의 node가 해당 조건을 충족하지 않고, 멈춘다면, 행동은 hold이다.

--- 마지막 결과값은 진입 포지션(long, short), 진입 비중(0 ~ 1의 실수), 레버리지 배수(0 ~ 100의 정수) 값들이 나와야 한다.
--- 여러 leaf 노드에서 조건이 만족했다면, 가장 먼저 탐색을 완료한 결과로 진행한다.
--- 마지막 결과값도 변이가 가능하다.

--- 해당 노드들은 무조건 고정된 노드들이며, 변이되지 않는다. 즉, 가장 위의 노드들은 변경되어서는 안된다.

너가 이해한 내용을 정리해서 작성해봐

---------

좋아 정확하게 이해했어. 이제 해당 방법의 TREE를 코드로 작성하되, CUDA 및 C 코드에서도 쉽게 동작할 수 있는 코드로 작성해줘.
즉, 해당 TREE의 연산이 C 및 CUDA에서도 이루어질 수 있도록 작성해줘.
그 이유는 ga를 학습시키기 위해서 CUDA를 활용할 계획이다.
나의 계획에 대해 간략하게 설명하자면,
만약 수천 수만개의 TREE가 POP_SIZE로 존재할 경우, 이것을 CPU로 연산하기에는 매우 오래 걸린다.
따라서, CUDA를 활용하여, 스레드 하나마다 TREE를 배정하여, 병렬 연산을 진행하여 추론을 진행할 예정이다.
따라서, 해당 상황에서도 적절하게 동작할 수 있는 모델을 작성해야 한다.
아직, CROSSOVER 등 작성할 필요는 없고, 모델 부분만 작성을 해줘.
Tree에는 max_node와 max_depth가 존재해야 한다.
Tree에는 추가로 2가지 기능이 추가되어야 한다. 처음 Tree를 생성할 때, 조건에 맞는 랜덤 Tree 생성
그리고 load() method를 통해 저장된 Tree weight? node?를 불러올 수 있는 함수도 작성해줘.

비교 연산은 3가지만 존재한다. > < =


강조하는 것은 여기서 말하는 cuda 및 C 호환의 의미는 Model 자체를 Cuda로 만드는 것이 목적이 아니다.
예를 들어 GATree와 그것을 인구단위로 묶은 GATreePop이라는 python Class를 만들었을 때, GATreePop 자체가 c와 cuda에 입력으로 들어와 연산을 하는 것이 목적이다.
즉, c에서 담당할 부분은 python으로 들어온 객체를 cuda kernel에 넣을 수 있는 형태로 변환하는 기능
cuda에서 담당할 부분은 병렬적으로 모든 Tree의 추론을 진행.
이를 위해서는 미리 Tree의 데이터 정보들을 GPU에 올려놓을 수 있도록 Torch.Tensor를 사용하자(grad는 필요없으므로 False)

가장 대표적인 예시로 python Torch에서 선언된 Tensor를 setup 및 커스텀 c와 cuda 함수를 통해 연산을 하는 방식이라고 보면된다.
load에는 2가지 입력을 받을 수 있다. 파일 경로 혹은 torch의 state_dict
따라서 저장 역시 torch의 state_dict 형식으로 저장하고, 해당 tree의 모든 정보를 저장해야 한다.

일단 코드 작성하지 말고 너가 이해한 내용을 먼저 정리해봐.
매우 많은 요구사항이기 때문에, 빠짐없이, 매우 자세하게 분석 후 정리해야 한다.

---------------------------------------------------------------------------------------------------------------_
요구사항
"""
"""
genetic tree를 구현해줘. 아래의 명세를 모두 반영해야 한다.
- 해당 GA tree의 목적은 자동 트레이딩을 위한 코드이다.
- tree의 분기는 무조건 비교 구문으로 구성된다.
-- tree의 분기는 무조건 2개의 형식으로 구분된다.
--- 하나의 노드의 자식 노드는 여러개가 될 수 있다. (자식 노드 제한 존재 - argument로 입력 예정)
--- feature >< number or feature1 >< feature2
--- 형식의 feature는 미리 지정해준다.

--- ex)   feature_num     = ['RSI', 'ATR', 'WR'] 
  feature_pair    = ['SMA', 'EMA', 'BB_upper','BB_lower']
--- 즉, 숫자와 비교하는 feature와 feature끼리 비교하는 feature들의 그룹을 미리 지정해줄 예정이다.
--- feature_num의 경우, 해당 데이터의 최대 및 최소 값도 같이 주어질 예정이다.
--- GA의 변이는 tree의 분기를 늘리거나 줄이거나 해당 분기를 위의 조건에 맞는 한에서 변경한다.
-- 트리의 root 바로 아래 노드는 3개의 노드가 존재하며, 각각 현재 포지션이 ['LONG', 'HOLD', 'SHORT']인지 구분한다.
--- crossover는 기존 트리에서 일정 부분의 branch 전체를 선택해서 crossover를 진행한다.
--- crossover되는 branch에 하위 branch가 존재한다면, 통째로 crossover가 된다.
--- 자식 노드가 여러개일 경우에는, 하나의 자식 노드만 충족해도 해당 자식 노드들의 모든 경우를 탐색한다.
---- 예를 들어 자식 노드 1, 2, 3이 있을 경우 1과 3의 경우가 충족되었다면, 1과 3의 tree로 진행한다.
--- 진행한 트리의 node가 해당 조건을 충족하지 않고, 멈춘다면, 행동은 hold이다.
--- 자식 노드로 Action 노드를 가지고 있는 경우, 무조건 하나의 자식 노드, 즉, 해당 Action 노드만 가지고 있어야 한다
--- 자식 노드에는 결정 노드와 행동 노드가 혼재되어 있으면 안된다. 결정 노드는 무조건 중간에 존재하는 노드이며, 행동노드는 무조건 마지막에만 존재하는 노드이다.
예시) - Decision A -- Action B and Action C --> 불가 Action node는 무조건 하나만 존재해야 함
 - - Decision A -- Action B and Decision C and Decision B -> 행동과 결정 노드가 자식 노드에서 혼재하는 것은 불가


--- 마지막 결과값은 진입 포지션(long, short), 진입 비중(0 ~ 1의 실수), 레버리지 배수(0 ~ 100의 정수) 값들이 나와야 한다.
--- 여러 leaf 노드에서 조건이 만족했다면, 가장 먼저 탐색을 완료한 결과로 진행한다.
--- 마지막 결과값도 변이가 가능하다.

--- 해당 노드들은 무조건 고정된 노드들이며, 변이되지 않는다. 즉, 가장 위의 노드들은 변경되어서는 안된다.

CUDA 및 C 코드에서도 쉽게 동작할 수 있어야 한다.
즉, 해당 TREE의 연산이 C 및 CUDA에서도 이루어질 수 있도록 작성해줘.
그 이유는 ga를 학습시키기 위해서 CUDA를 활용할 계획이다.
나의 계획에 대해 간략하게 설명하자면,
만약 수천 수만개의 TREE가 POP_SIZE로 존재할 경우, 이것을 CPU로 연산하기에는 매우 오래 걸린다.
따라서, CUDA를 활용하여, 스레드 하나마다 TREE를 배정하여, 병렬 연산을 진행하여 추론을 진행할 예정이다.
따라서, 해당 상황에서도 적절하게 동작할 수 있는 모델을 작성해야 한다.
Tree에는 max_node와 max_depth가 존재해야 한다.
Tree에는 추가로 2가지 기능이 추가되어야 한다. 처음 Tree를 생성할 때, 조건에 맞는 랜덤 Tree 생성
그리고 load() method를 통해 저장된 Tree weight? node?를 불러올 수 있는 함수도 작성해줘.

GATree의 노드는 2가지 받을 수 있도록 한다. GATree 하나만을 사용할 수 있도록 기존 Tensor를 Tree Class내에 선언하는 경우, (값으로 존재)
GATreePop에서 선언한 Tensor를 주소형태(torch.view 사용)로 일부만 가져와서 존재.
-> 해당 형태로 구현한 이유에 대해서도 너가 추측해서 설명해봐

비교 연산은 3가지만 존재한다. > < =


강조하는 것은 여기서 말하는 cuda 및 C 호환의 의미는 Model 자체를 Cuda로 만드는 것이 목적이 아니다.
예를 들어 GATree와 그것을 인구단위로 묶은 GATreePop이라는 python Class를 만들었을 때, GATreePop 자체가 c와 cuda에 입력으로 들어와 연산을 하는 것이 목적이다.
즉, c에서 담당할 부분은 python으로 들어온 객체를 cuda kernel에 넣을 수 있는 형태로 변환하는 기능
cuda에서 담당할 부분은 병렬적으로 모든 Tree의 추론을 진행.
이를 위해서는 미리 Tree의 데이터 정보들을 GPU에 올려놓을 수 있도록 Torch.Tensor를 사용하자(grad는 필요없으므로 False)

가장 대표적인 예시로 python Torch에서 선언된 Tensor를 setup 및 커스텀 c와 cuda 함수를 통해 연산을 하는 방식이라고 보면된다.
load에는 2가지 입력을 받을 수 있다. 파일 경로 혹은 torch의 state_dict
따라서 저장 역시 torch의 state_dict 형식으로 저장하고, 해당 tree의 모든 정보를 저장해야 한다.

처음 GATree 및 GATreePop 생성시 Node 등 초기화하지 않고, __init___tree()라는 method를 사용해야 Node 등 Tree를 초기화하도록 해.
가장 중요하고 중요한 점은 Tree 초기화시 지금까지 설명한 제약 사항 및 구현 사항들을 모두 고려해야 한다.
따라서 어떻게 초기화를 하고, 어떤 제약 사항 및 구현사항들을 고려해서 초기화 방법을 설정했는지도 매우 자세하게 설명해줘.

매우 많은 요구사항이기 때문에, 빠짐없이, 매우 자세하게 분석 후 정리해야 한다.
너가 이해한 내용을 정리해서 작성해봐
예시로 임시의 Tree를 만들어서 설명해봐.

코드 작성하지 말고, 매우 길고 자세하고 정교한 분석 설명해줘
"""


첨부된 코드는 위의 요구사항을 바탕으로 작성한 코드이다. 이 때, 첨부된 파일 중, 보다시피 muation의 경우, 세부적인 방법론 구현을 아직 진행하지 않았다. 일단 먼저 mutation 방법론들을 어떻게 구현할지 설계해줘.

다시 한번 더 말하지만, 일단 구현 방법 설계를 진행해. 내가 검토한 후, 좋다면 후에 코드 작성 명령을 줄게.
추가로 말하자면, 현재 mutation는 python 함수에서 이뤄지도록 할 예정이다. --> 복잡한 연산으로 판단하여, c cuda가 효율적인지 의문
"""

보고서
"""
## GATree 변이(Mutation) 연산자 구현 상세 설계 보고서

본 문서는 자동 트레이딩을 위한 `GATree`의 유전 알고리즘 변이 연산자 구현을 위한 상세 설계안입니다. 사용자의 새로운 요구사항(구조 변경 세분화, 벡터화된 확률적 적용, 유효성 검사 강화)을 모두 반영하여 기존 설계안을 확장 및 재구성했습니다. 이 보고서는 실제 코드 작성을 위한 완벽한 청사진 역할을 하는 것을 목표로 합니다.

### 1. 변이 연산의 공통 설계 철학

모든 변이 연산자는 다음의 4대 원칙을 철저히 준수하여 C++/CUDA 호환성 및 알고리즘의 안정성을 보장합니다.

1.  **불변성 (Immutability)**: 원본 `chromosomes` 텐서(`(N, max_nodes, node_dim)`)는 절대 직접 수정하지 않습니다. 항상 `clone()`을 통해 복사본을 생성하고, 그 복사본에 변이를 적용한 후 새로운 텐서를 반환합니다.
2.  **확률적 적용 (Vectorized Probability)**: 각 변이 연산자는 개별 노드 또는 개체에 대해 확률적으로 적용됩니다. Python의 `for` 루프 성능 저하를 피하기 위해, **벡터화된 연산**을 통해 변이를 적용할 대상을 일괄적으로 결정합니다.
    - **핵심 메커니즘**: `torch.rand(N, max_nodes) < self.prob` 와 같은 방식으로 변이 대상 마스크(mask)를 생성하고, 이 마스크가 `True`인 위치의 노드에 대해서만 실제 변이 로직을 수행합니다.
3.  **유효성 보장 (Validity Guarantee)**: 모든 변이 연산은 실행 전후로 트리의 제약 조건(`max_nodes`, `max_depth`, `max_children`, 노드 연결 규칙 등)을 반드시 검사합니다. 변이로 인해 유효하지 않은 트리가 생성될 경우, 해당 변이는 취소되고 원본 상태가 유지됩니다.
4.  **배치 처리 (Batch Processing)**: 모든 연산자의 `__call__` 메소드는 단일 트리가 아닌, 집단 전체를 나타내는 3D 텐서(`chromosomes`)를 입력으로 받아 처리합니다.

---

### 2. 값 변경 (Value-based) 변이 연산자

트리의 구조는 그대로 유지한 채, 노드 내부의 파라미터 값만 변경하여 해의 정밀 탐색(Exploitation)을 수행합니다.

#### 2.1. `NodeParamMutation` (노드 파라미터 미세 조정)

-   **역할 및 목표**: 기존 해의 근방을 세밀하게 탐색하기 위해 노드 파라미터를 작은 폭으로 변경합니다.
-   **초기화 인자**: `__init__(self, prob: float)`
-   **구현 설계 (벡터화 기반)**:
    1.  **대상 노드 마스크 생성**:
        a.  변이가능 노드 마스크 생성: `ACTION` 또는 `DECISION` 타입인 노드.
            `target_type_mask = (chromosomes[:, :, COL_NODE_TYPE] == NODE_TYPE_DECISION) | (chromosomes[:, :, COL_NODE_TYPE] == NODE_TYPE_ACTION)`
        b.  확률 적용 마스크 생성:
            `prob_mask = torch.rand_like(chromosomes[:, :, 0]) < self.prob`
        c.  **최종 변이 대상 마스크**:
            `mutation_mask = target_type_mask & prob_mask`
        d.  변이 대상 인덱스 추출:
            `mutation_indices = mutation_mask.nonzero()` `(num_mutations, 2)` shape.
    2.  **선택적 순회 및 변이 적용**: `mutation_indices`를 순회하며 각 `(chrom_idx, node_idx)`에 대해 변이를 적용합니다. (이 루프는 전체 노드를 순회하는 것보다 훨씬 효율적입니다.)
        -   **`ACTION` 노드**: 파라미터 중 하나를 무작위로 선택 후 변경.
            -   `COL_PARAM_1` (포지션): `LONG` ↔ `SHORT` 토글.
            -   `COL_PARAM_2` (비중): `val += N(0, 0.1)`, `[0, 1]` 범위로 클램핑.
            -   `COL_PARAM_3` (레버리지): `val += randint(-5, 5)`, `[1, 100]` 범위로 클램핑.
        -   **`DECISION` 노드**: 변경할 요소 중 하나를 무작위로 선택 후 변경.
            -   `COL_PARAM_2` (연산자): 현재와 다른 연산자를 무작위 선택.
            -   `COL_PARAM_1`, `COL_PARAM_4` (피처/값):
                -   `FEAT_NUM`: `COL_PARAM_4` 값을 **피처의 전체 범위에 비례하는 노이즈**로 변경. `noise = uniform(-0.1, 0.1) * (max_val - min_val)`.
                -   `FEAT_FEAT`: 피처 중 하나를 다른 피처로 교체 (단, 두 피처가 같아지지 않도록. 그리고 Feat 끼리 비교하는 Feat에서 가져올 것).

#### 2.2. `ReinitializeNodeMutation` (노드 완전 초기화)

-   **역할 및 목표**: 지역 최적해 탈출을 위해 특정 노드의 파라미터를 완전히 새로운 랜덤 값으로 교체합니다.
-   **초기화 인자**: `__init__(self, prob: float)`
-   **구현 설계**:
    1.  **대상 노드 선택**: `NodeParamMutation`과 동일한 벡터화 방식으로 `mutation_indices`를 구합니다.
    2.  **선택적 순회 및 재초기화**: `mutation_indices`를 순회하며 다음을 수행합니다.
        a.  선택된 노드 `(chrom_idx, node_idx)`의 핵심 구조 정보(`node_type`, `parent_idx`, `depth`)를 임시 저장합니다.
        b.  해당 노드의 텐서 행(row)을 0으로 초기화합니다.
        c.  저장해둔 `node_type`, `parent_idx`, `depth`를 다시 기록합니다.
        d.  `node_type`에 따라 `GATree`의 **파라미터 생성 헬퍼 메소드**(`_generate_random_action_params`, `_generate_random_decision_params`)를 호출하여 완전히 새로운 파라미터를 생성하고, 텐서에 기록합니다.

---

### 3. 구조 변경 (Structure-based) 변이 연산자

트리의 복잡도를 직접적으로 변경하여 새로운 해를 탐색(Exploration)합니다.

#### 3.1. `DeleteNodeMutation` (단일 노드 삭제)

-   **역할 및 목표**: 트리의 논리적 흐름을 단순화하기 위해 중간 `Decision` 노드 하나를 제거하고, 그 자식들을 조부모 노드에 직접 연결합니다 (Splicing).
-   **초기화 인자**: `__init__(self, prob: float)`
-   **구현 설계**:
    1.  **대상 노드 선택**:
        a.  삭제 가능 대상: `DECISION` 타입이고, `ROOT_BRANCH`의 직접적인 자식이 아닌 노드.
        b.  `NodeParamMutation`과 동일한 벡터화 방식으로 `mutation_indices`를 구합니다.
    2.  **선택적 순회 및 삭제**: `mutation_indices`를 순회하며 다음을 수행합니다.
        a.  삭제할 노드(`D`)의 부모(`P`)와 자식들(`C1, C2, ...`)의 인덱스를 식별합니다.
        b.  **유효성 검사**: `P`의 자식 수(`num_children(P)`)와 `D`의 자식 수(`num_children(D)`)의 합이 `max_children + 1`을 초과하는지 확인합니다. (D가 사라지고 D의 자식들이 P에게 가므로) 초과하면 변이 취소.
        c.  **재연결 (Re-wiring)**: 자식(`C1, C2, ...`)들의 `COL_PARENT_IDX`를 `P`의 인덱스로 변경합니다.
        d.  **깊이 업데이트**: `C1, C2, ...` 와 그 모든 자손들의 `COL_DEPTH`를 1씩 감소시킵니다. (각 자식 `C`로부터 시작하는 BFS/DFS 필요)
        e.  삭제할 노드 `D`의 행을 0으로 만들고 `NODE_TYPE_UNUSED`로 설정합니다.

#### 3.2. `DeleteSubtreeMutation` (서브트리 삭제)

-   **역할 및 목표**: 불필요하거나 성능이 낮은 규칙 뭉치(가지)를 통째로 제거하여 트리를 대폭 단순화합니다.
-   **초기화 인자**: `__init__(self, prob: float)`
-   **구현 설계**:
    1.  **대상 노드 선택**:
        a.  삭제 가능 대상: `ROOT_BRANCH`가 아닌 모든 활성 노드.
        b.  벡터화 방식으로 `mutation_indices` (삭제될 서브트리의 루트 목록)를 구합니다.
    2.  **선택적 순회 및 삭제**: `mutation_indices`를 순회하며 다음을 수행합니다.
        a.  선택된 서브트리 루트(`R`)로부터 시작하는 BFS/DFS 탐색으로 삭제할 모든 노드 인덱스 리스트를 구합니다.
        b.  이 리스트에 포함된 모든 노드의 행을 0으로 만들고 `NODE_TYPE_UNUSED`로 설정합니다.
        c.  **후처리 (고아 부모 문제 해결)**:
            -   삭제된 서브트리의 루트(`R`)의 부모 노드(`P`)를 확인합니다.
            -   이 삭제로 인해 `P`의 자식이 모두 사라졌고, `P`가 `DECISION` 노드라면, 트리의 논리적 완결성을 위해 `P`의 자식으로 **새로운 랜덤 `Action` 노드를 추가**합니다. (단, 전체 트리에 빈 슬롯이 남아있을 경우)

#### 3.3. `AddNodeMutation` (단일 노드 추가)

-   **역할 및 목표**: 기존 규칙 경로 중간에 새로운 `Decision` 조건을 삽입하여 규칙을 더 세분화합니다.
-   **초기화 인자**: `__init__(self, prob: float)`
-   **구현 설계**:
    1.  **대상 "엣지" 선택**:
        a.  삽입 가능 대상: 부모-자식 연결(엣지). 즉, `ROOT_BRANCH`가 아닌 모든 자식 노드가 대상이 될 수 있습니다.
        b.  벡터화 방식으로 `mutation_indices` (삽입될 위치의 기존 자식 노드 목록)를 구합니다.
    2.  **선택적 순회 및 삽입**: `mutation_indices`를 순회하며 다음을 수행합니다.
        a.  삽입 위치의 부모(`P`)와 자식(`C`) 노드를 식별합니다.
        b.  **유효성 검사**: `P`의 깊이가 `max_depth - 2` 보다 작은지, 전체 트리에 빈 슬롯이 있는지 확인합니다. (새 노드와 기존 자식, 2단계 깊이 필요)
        c.  사용 가능한 빈 슬롯을 찾아 새 노드(`N`)의 인덱스로 할당합니다.
        d.  **재연결**:
            -   새 노드 `N` 생성: `parent_idx`는 `P`로, `depth`는 `P`의 깊이+1로 설정. 랜덤 `Decision` 파라미터 생성.
            -   기존 자식 `C`의 `parent_idx`를 `N`의 인덱스로 변경합니다.
        e.  **깊이 업데이트**: `C`와 그 모든 자손들의 `COL_DEPTH`를 1씩 증가시킵니다. (C로부터 시작하는 BFS/DFS 필요)

#### 3.4. `AddSubtreeMutation` (서브트리 추가)

-   **역할 및 목표**: 트리에 새로운 규칙 뭉치(가지)를 추가하여 복잡도를 높이고, 새로운 탐색 공간을 개척합니다.
-   **초기화 인자**: `__init__(self, prob: float, node_count_range: tuple = (2, 5))`
-   **구현 설계**:
    1.  **대상 "부모" 선택**:
        a.  추가 가능 대상: 자식 수가 `max_children` 미만이고, 깊이가 `max_depth`에 가깝지 않으며, 자식으로 `Action` 노드를 갖지 않는 `DECISION` 노드.
        b.  벡터화 방식으로 `mutation_indices` (서브트리가 추가될 부모 노드 목록)를 구합니다.
    2.  **선택적 순회 및 추가**: `mutation_indices`를 순회하며 다음을 수행합니다.
        a.  추가할 서브트리의 노드 수(`budget`)를 `node_count_range` 내에서 랜덤하게 결정합니다.
        b.  **유효성 검사**: 전체 트리에 `budget` 만큼의 빈 슬롯이 있는지, 추가될 서브트리가 `max_depth`를 초과하지 않는지 확인합니다.
        c.  **서브트리 성장**: `GATree._grow_branch`와 유사한 로직을 실행하여, 선택된 부모 노드 아래에 `budget` 크기의 랜덤 서브트리를 생성합니다. 이 과정에서 모든 새 노드의 `parent_idx`와 `depth`가 정확하게 계산되어야 합니다.

---
### 4. 종합 및 결론

본 설계안은 기존 설계를 기반으로 사용자의 피드백을 완벽하게 통합한, 코드 구현을 위한 최종 명세서입니다. 각 변이 연산자는 명확한 역할(탐색/활용, 복잡도 증가/감소)을 가지며, 벡터화된 확률 적용 방식을 통해 Python 레벨에서의 성능 저하를 최소화합니다. 특히 구조 변경 연산자를 단일 노드와 서브트리 단위로 세분화하고, 각 경우에 발생할 수 있는 엣지 케이스(고아 부모, 깊이/자식 수 초과)에 대한 해결책을 명시하여 알고리즘의 강건함을 크게 향상시켰습니다. 이 보고서를 바탕으로 코드 작성을 진행하면, 요구사항을 모두 만족하는 고성능, 고신뢰성의 변이 모듈을 구현할 수 있을 것입니다.
"""

해당 요구사항 및 정리문 그리고 주어진 코드를 바탕으로 코드를 작성하여라.
Mutation 코드들을 작성하여라.
